# -*- coding: utf-8 -*-
"""COVID19 CNN_Visualization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TESMHIR6-FJ-yUsV-a3CPwNr3tv05jF2

# basic
"""

import os
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import cv2
from PIL import Image

from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Conv2D, MaxPool2D, Flatten
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

#filtering warnings
import warnings
warnings.filterwarnings('ignore')

# global parameters
IMG_SIZE=224
IMAGE_CHANNELS=3
batch_size = 32
epochs = 20
validation_split = 0.3

"""# Image Preprocessing"""

# Use the following cell to connect to google drive to access the data strored in the x_ray folder
from google.colab import drive
drive.mount('/content/drive')

# Please change this path to the folder that contains both the training and testing images.
os.chdir('/content/drive/My Drive/STAT4609 Project/xray_dataset_covid19')

# Split the Images of the training set into those that are not infected with Corona (NI) 
# and those that are infected (I)
train_NI = os.listdir("train/NORMAL")
train_I = os.listdir("train/PNEUMONIA")

# Split the Images of the test set into Infected (I) and Not Infected (NI).
test_NI = os.listdir("test/NORMAL")
test_I = os.listdir("test/PNEUMONIA")

# put all path together in a dataframe

def dataframe(filenames, classes):
    return pd.DataFrame({'Filename':filenames, 'Class':[classes]*len(filenames)})

# Forming Train and Test Dataframe
train_df_NI = dataframe(train_NI, 'Normal')
train_df_I = dataframe(train_I, 'Covid')

test_df_NI = dataframe(test_NI, 'Normal')
test_df_I = dataframe(test_I, 'Covid')

#Combine both classes into a single datarame
df_train = pd.concat([train_df_NI, train_df_I], axis=0, ignore_index=True)
df_test = pd.concat([test_df_NI, test_df_I], axis=0, ignore_index=True)
df_train.head()

# In case there are files labeled "Thumbs.db" in the train and test dataframe
# which need to be removed as they are not images

# Examine the occurences of the "Thumbs.db" file
print("The number of .db file occurences in the training data:",len(df_train[df_train["Filename"] == "Thumbs.db"]))
print("The number of .db file occurences in the testing data:",len(df_test[df_test["Filename"] == "Thumbs.db"]))

# Remove the occurences of the "Thumbs.db"
df_train = df_train[df_train["Filename"] != "Thumbs.db"]
df_test = df_test[df_test["Filename"] != "Thumbs.db"]

# Remove all the Non-Covid-19 illnesses from the training and testing data.

# Removing the Non-Covid-19 images from the training data.
df_train.drop(df_train.index[df_train['Filename'] == "pneumocystis-pneumonia-2-PA.png"], inplace = True)
df_train.drop(df_train.index[df_train['Filename'] == "MERS-CoV-1-s2.0-S0378603X1500248X-gr4e.jpg"], inplace = True)
df_train.drop(df_train.index[df_train['Filename'] == "ARDSSevere.png"], inplace = True)
df_train.drop(df_train.index[df_train['Filename'] == "ards-secondary-to-tiger-snake-bite.png"], inplace = True)
df_train.drop(df_train.index[df_train['Filename'] == "acute-respiratory-distress-syndrome-ards-1.jpg"], inplace = True)

# Removing the Non-Covid-19 images from the testing data.
df_test.drop(df_test.index[df_test['Filename'] == "streptococcus-pneumoniae-pneumonia-temporal-evolution-1-day3.jpg"], inplace = True)
df_test.drop(df_test.index[df_test['Filename'] == "streptococcus-pneumoniae-pneumonia-temporal-evolution-1-day2.jpg"], inplace = True)
df_test.drop(df_test.index[df_test['Filename'] == "streptococcus-pneumoniae-pneumonia-temporal-evolution-1-day1.jpg"], inplace = True)
df_test.drop(df_test.index[df_test['Filename'] == "streptococcus-pneumoniae-pneumonia-temporal-evolution-1-day0.jpg"], inplace = True)
df_test.drop(df_test.index[df_test['Filename'] == "streptococcus-pneumoniae-pneumonia-1.jpg"], inplace = True)
df_test.drop(df_test.index[df_test['Filename'] == "SARS-10.1148rg.242035193-g04mr34g09c-Fig9c-day27.jpeg"], inplace = True)
df_test.drop(df_test.index[df_test['Filename'] == "SARS-10.1148rg.242035193-g04mr34g09b-Fig9b-day19.jpeg"], inplace = True)
df_test.drop(df_test.index[df_test['Filename'] == "SARS-10.1148rg.242035193-g04mr34g09a-Fig9a-day17.jpeg"], inplace = True)
df_test.drop(df_test.index[df_test['Filename'] == "SARS-10.1148rg.242035193-g04mr34g07b-Fig7b-day12.jpeg"], inplace = True)
df_test.drop(df_test.index[df_test['Filename'] == "SARS-10.1148rg.242035193-g04mr34g07a-Fig7a-day5.jpeg"], inplace = True)
df_test.drop(df_test.index[df_test['Filename'] == "SARS-10.1148rg.242035193-g04mr34g05x-Fig5-day9.jpeg"], inplace = True)
df_test.drop(df_test.index[df_test['Filename'] == "SARS-10.1148rg.242035193-g04mr34g04b-Fig4b-day12.jpeg"], inplace = True)
df_test.drop(df_test.index[df_test['Filename'] == "SARS-10.1148rg.242035193-g04mr34g04a-Fig4a-day7.jpeg"], inplace = True)
df_test.drop(df_test.index[df_test['Filename'] == "SARS-10.1148rg.242035193-g04mr34g0-Fig8c-day10.jpeg"], inplace = True)
df_test.drop(df_test.index[df_test['Filename'] == "SARS-10.1148rg.242035193-g04mr34g0-Fig8b-day5.jpeg"], inplace = True)
df_test.drop(df_test.index[df_test['Filename'] == "SARS-10.1148rg.242035193-g04mr34g0-Fig8a-day0.jpeg"], inplace = True)

df_train.reset_index(drop=True, inplace=True)
df_test.reset_index(drop=True, inplace=True)
# Verify that are no more occurences
print("The number of .db file occurences in the training data:",len(df_train[df_train["Filename"] == "Thumbs.db"]))
print("The number of .db file occurencesin the testing data:",len(df_test[df_test["Filename"] == "Thumbs.db"]))

def pathmaker(df,folder):
    path_list = []
    for i in df.values:
        if i[1] == 'Covid':
            path_list.append(str(folder+'/PNEUMONIA/'+i[0]))
        else:
            path_list.append(str(folder+'/NORMAL/'+i[0]))
    return path_list

# Assigning Path maker
df_train['Path'] =  pathmaker(df_train,'train')
df_test['Path'] =  pathmaker(df_test,'test')

# Visualize the df_train to ensure that the paths were appended correctly
pd.set_option('display.max_colwidth', -1)
df_train

df_test

# Plotting Normal VS Covid in Grid

sns.set_context('talk')
plt.figure(figsize=(12,12))
plt.subplots_adjust(hspace=0.3)

plt.subplot(2, 2, 1)
plt.imshow(mpimg.imread(df_train[df_train['Class'] == 'Normal']['Path'].values[0]),cmap="gray")
plt.title('Normal')

plt.subplot(2, 2, 2)
plt.imshow(mpimg.imread(df_train[df_train['Class'] == 'Normal']['Path'].values[6]),cmap="gray")
plt.title('Normal')

plt.subplot(2, 2, 3)
plt.imshow(mpimg.imread(df_train[df_train['Class'] == 'Covid']['Path'].values[28]),cmap="gray")
plt.title('COVID-19')

plt.subplot(2, 2, 4)
plt.imshow(mpimg.imread(df_train[df_train['Class'] == 'Covid']['Path'].values[26]),cmap="gray")
plt.title('COVID-19')

# problems of using raw data (before processing) for CNN

print('different images have different size')
print('also the shape is automatically recognised by the package')
print('gray image may not be gray-scaled, some have dimension (width, height, 3)')
img0 = mpimg.imread(df_train['Path'][0])
img1 = mpimg.imread(df_train['Path'][3])
img2 = mpimg.imread(df_test['Path'][22])
img3 = mpimg.imread(df_test['Path'][23])

fig, axs = plt.subplots(2, 2, figsize=(12,12))

axs[0,0].imshow(img0,cmap="gray")
axs[0,0].set_title('shape: {}'.format(img0.shape))

axs[0,1].imshow(img1,cmap="gray")
axs[0,1].set_title('shape: {}'.format(img1.shape))

axs[1,0].imshow(img2,cmap="gray")
axs[1,0].set_title('shape: {}'.format(img2.shape))

axs[1,1].imshow(img3,cmap="gray")
axs[1,1].set_title('shape: {}'.format(img3.shape))

plt.show()

# Pre-processing
# 1. resize all images
# 2. convert all images to have the same dimension (width, height, 3) as 'RGB' images
# note that images in keras are represented in 'RGB' color sequence

def image_preprocessor(path, desired_size=IMG_SIZE):
    im = Image.open(path).convert('RGB')
    im = im.resize((desired_size,desired_size), Image.LANCZOS)
    return im

# training set
x_train = np.empty((len(df_train), IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)

for i, path in enumerate(tqdm(df_train['Path'])):
    x_train[i,:,:,:] = image_preprocessor(path)
    
# testing set
x_test = np.empty((len(df_test), IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)

for i, path in enumerate(tqdm(df_test['Path'])):
    x_test[i,:,:,:] = image_preprocessor(path)
    
x_train = x_train/255.0
x_test = x_test/255.0

# some images after processing
# comparing with former examples

fig, axs = plt.subplots(2, 2, figsize=(12,12))

axs[0,0].imshow(x_train[0])
axs[0,0].set_title('shape: {}'.format(x_train[0].shape))

axs[0,1].imshow(x_train[3])
axs[0,1].set_title('shape: {}'.format(x_train[3].shape))

axs[1,0].imshow(x_test[22])
axs[1,0].set_title('shape: {}'.format(x_test[22].shape))

axs[1,1].imshow(x_test[23])
axs[1,1].set_title('shape: {}'.format(x_test[23].shape))

plt.show()

# Convert the "Class" collumn of the df_train and df_test dataframes into a binary variable
# with 0 indicting Normal and 1 indicating COVID

df_train['Class'] = df_train['Class'].replace(['Covid','Normal'], [1,0])
df_test['Class'] = df_test['Class'].replace(['Covid','Normal'], [1,0])

#defining output layer's output

y_train = to_categorical(df_train['Class'], num_classes=2)
y_test = to_categorical(df_test['Class'], num_classes=2)

# Data Augmentation

datagen = image.ImageDataGenerator(
    #rescale=1./255,
    vertical_flip = False,
    shear_range = 0,
    zoom_range = 0,
        
    horizontal_flip = True,
    rotation_range=10,
    width_shift_range=0.05,
    height_shift_range=0.05,
    fill_mode='nearest',
    
    validation_split=validation_split)    
    
datagen.fit(x_train, augment=True)
generator_train = datagen.flow(x_train, y_train, subset='training', shuffle=True, batch_size=batch_size)
generator_valid = datagen.flow(x_train, y_train, subset='validation', shuffle=True, batch_size=batch_size)

# visualize the data

plt.figure(figsize=(18, 30))

for i in range(0, 15):
    plt.subplot(5, 3, i+1)
    for X_batch, Y_batch in generator_train:
        image = X_batch[0]
        plt.imshow(image)
        break

plt.tight_layout()
plt.show()

# additional remark: Category count Plot
sns.countplot(df_train['Class'], palette='BrBG')
plt.title('Category Count')

"""# Building the CNN (fit on original data)"""

from tensorflow.keras.layers import Activation, Dropout, Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization
model = Sequential()

#Normalization & Input
model.add(BatchNormalization(input_shape=(IMG_SIZE, IMG_SIZE, 3)))

#CNN
model.add(Conv2D(32, kernel_size=(3,3), padding='same', activation='relu'))

#Max Pooling
model.add(MaxPool2D(pool_size=(2,2)))

#Dropout
model.add(Dropout(0.25))

#Flattening I/P
model.add(Flatten())

#Dense layer
model.add(Dense(128, activation='relu'))

#Dropout
model.add(Dropout(0.1))

#Output Layer with sigmoid
model.add(Dense(2, activation='sigmoid'))

#Adam as optimizer, Accuracy as evaluation metrics
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# overview of deep neural network
model.summary()

# Saving best model while monitoring accuracy
model_chkpt = ModelCheckpoint('/content/drive/My Drive/STAT4609/STAT4609_Group_Project/',
                              save_best_only=True, monitor='accuracy')

# early stopping for preventing overfitting
early_stopping = EarlyStopping(monitor='loss', restore_best_weights=False, patience=5)

# Model fitting and tracking historical accuracy / error score
history = model.fit(x_train, y_train, 
          validation_split=validation_split, 
          epochs=epochs, batch_size=batch_size, shuffle=True, 
          callbacks=[model_chkpt, early_stopping]
         )

# plotting Learning curve over No. of Iterations
sns.set_context('talk')
plt.figure(figsize=(10,7))
plt.plot(history.history['loss'], 'b', label='Training', marker='x')
plt.plot(history.history['val_loss'], 'r', label='Validation', marker='o')
plt.legend()
plt.title('Learning Curve')
plt.xlabel('Iterations')
plt.ylabel('Error')

# prediction on test set
pred = model.predict(x_test, batch_size=batch_size)
# print(pred)

# converting Probabilities to labels
label = np.argmax(pred, axis=1)
label

# Accuracy Score
print ('Accuracy Score : ', accuracy_score(label, df_test['Class'].values), '\n')

# precision, recall report
print ('Classification Report :\n\n' ,classification_report(label, df_test['Class'].values))

# plotting confusion matrix
sns.heatmap(confusion_matrix(label, df_test['Class'].values), annot=True, cmap='inferno_r', square=True)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Confusion Matrix')

"""# Building the CNN (fit on augmented data)"""

model = Sequential()

#Normalization & Input
model.add(BatchNormalization(input_shape=(IMG_SIZE, IMG_SIZE, 3)))

#CNN
model.add(Conv2D(32, kernel_size=(3,3), padding='same', activation='relu'))

#Max Pooling
model.add(MaxPool2D(pool_size=(2,2)))

#Dropout
model.add(Dropout(0.25))

#Flattening I/P
model.add(Flatten())

#Dense layer
model.add(Dense(128, activation='relu'))

#Dropout
model.add(Dropout(0.1))

#Output Layer with sigmoid
model.add(Dense(2, activation='sigmoid'))

#Adam as optimizer, Accuracy as evaluation metrics
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Saving best model while monitoring accuracy
model_chkpt = ModelCheckpoint('/content/drive/My Drive/STAT4609/STAT4609_Group_Project/',
                              save_best_only=True, monitor='accuracy')

# early stopping for preventing overfitting
early_stopping = EarlyStopping(monitor='loss', restore_best_weights=False, patience=5)

# Model fitting and tracking historical accuracy / error score
history = model.fit_generator(generator_train,
                              steps_per_epoch=len(x_train)*2/batch_size,
                              validation_data=generator_valid,
                              validation_steps=generator_valid.n//generator_valid.batch_size,
                              epochs=epochs,
                              callbacks=[model_chkpt, early_stopping])

# plotting Learning curve over No. of Iterations
sns.set_context('talk')
plt.figure(figsize=(10,7))
plt.plot(history.history['loss'], 'b', label='Training', marker='x')
plt.plot(history.history['val_loss'], 'r', label='Validation', marker='o')
plt.legend()
plt.title('Learning Curve')
plt.xlabel('Iterations')
plt.ylabel('Error')

# prediction on test set
pred = model.predict(x_test, batch_size=32)
# print(pred)

# converting Probabilities to labels
label = np.argmax(pred, axis=1)
label

# Accuracy Score
print ('Accuracy Score : ', accuracy_score(label, df_test['Class'].values), '\n')

# precision, recall report
print ('Classification Report :\n\n' ,classification_report(label, df_test['Class'].values))

# plotting confusion matrix
sns.heatmap(confusion_matrix(label, df_test['Class'].values), annot=True, cmap='inferno_r', square=True)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Confusion Matrix')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np

from keras.utils import np_utils
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dropout, Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.activations import relu

# %matplotlib inline
import matplotlib.pyplot as plt
def iter_occlusion(image, size=8):

    occlusion = np.full((size * 5, size * 5, 1), [0.5], np.float32)
    occlusion_center = np.full((size, size, 1), [0.5], np.float32)
    occlusion_padding = size * 2

    # print('padding...')
    image_padded = np.pad(image, ( \
                        (occlusion_padding, occlusion_padding), (occlusion_padding, occlusion_padding), (0, 0) \
                        ), 'constant', constant_values = 0.0)

    for y in range(occlusion_padding, image.shape[0] + occlusion_padding, size):

        for x in range(occlusion_padding, image.shape[1] + occlusion_padding, size):
            tmp = image_padded.copy()

            tmp[y - occlusion_padding:y + occlusion_center.shape[0] + occlusion_padding, \
                x - occlusion_padding:x + occlusion_center.shape[1] + occlusion_padding] \
                = occlusion

            tmp[y:y + occlusion_center.shape[0], x:x + occlusion_center.shape[1]] = occlusion_center

            yield x - occlusion_padding, y - occlusion_padding, \
                  tmp[occlusion_padding:tmp.shape[0] - occlusion_padding, occlusion_padding:tmp.shape[1] - occlusion_padding]

from tensorflow.keras.preprocessing.image import load_img
# load an image from file
#load_img(("/content/drive/My Drive/STAT4609 Project/xray_dataset_covid19/test/PNEUMONIA/ryct.2020200034.fig5-day7.jpeg", target_size=(224, 224))
#mpimg.imread(df_test['Path'][22])

image = x_test[23]
plt.imshow(image)
plt.title('Image of infected individual`s lungs \n')

from tensorflow.keras.preprocessing.image import img_to_array
# convert the image pixels to a numpy array
image = img_to_array(image)
# reshape data for the model
image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
# prepare the image for the VGG model
#image = preprocess_input(image)
# predict the probability across all output classes
yhat = model.predict(image)
temp = image[0]
print(temp.shape)
heatmap = np.zeros((224,224))
correct_class = np.argmax(yhat)
print(correct_class)
for n,(x,y,image) in enumerate(iter_occlusion(temp,14)):
    heatmap[x:x+14,y:y+14] = model.predict(image.reshape((1, image.shape[0], image.shape[1], image.shape[2])))[0][correct_class]
    #print(x,y,n,' - ',image.shape)
heatmap1 = heatmap/np.max(heatmap)
plt.figure(figsize=(20,10))
plt.title('Heatmap of infected individual`s lungs \n')
plt.imshow(heatmap1)

print(heatmap1)

import skimage.io as io
#creating mask from the standardised heatmap probabilities
mask = heatmap1 > np.quantile(heatmap1,0.5)
mask1 = mask *256
mask = mask.astype(int)
#io.imshow(mask1,cmap='gray')
io.imshow(mask,cmap='gray')

print(np.quantile(heatmap1,0.85))
print(np.quantile(heatmap1,0.15))

import cv2
#read the image
image = x_test[10]
#image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)
#resize image to appropriate dimensions
#image = cv2.resize(image,(224,224))
mask = mask.astype('uint8')
#apply the mask to the image
final = cv2.bitwise_and(image,image,mask = mask)
final = cv2.cvtColor(final,cv2.COLOR_BGR2RGB)
#plot the final image
plt.figure(figsize=(20,10))
plt.title("Image of Infected individual`s (22) lung showing only the top 50% of important features\n")
plt.imshow(final)

image = x_test[19]
plt.imshow(image)
plt.title('ORIGINAL IMAGE')
image = img_to_array(image)
# reshape data for the model
image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
# prepare the image for the VGG model
#image = preprocess_input(image)
# predict the probability across all output classes
yhat = model.predict(image)
temp = image[0]
print(temp.shape)
heatmap = np.zeros((224,224))
correct_class = np.argmax(yhat)
print(correct_class)
for n,(x,y,image) in enumerate(iter_occlusion(temp,14)):
    heatmap[x:x+14,y:y+14] = model.predict(image.reshape((1, image.shape[0], image.shape[1], image.shape[2])))[0][correct_class]
    #print(x,y,n,' - ',image.shape)
heatmap1 = heatmap/np.max(heatmap)
plt.figure(figsize=(20,10))
plt.imshow(heatmap)

mask = heatmap1 > np.quantile(heatmap1,0.15)
mask1 = mask *256
mask = mask.astype(int)
#io.imshow(mask1,cmap='gray')
io.imshow(mask,cmap='gray')
image = x_test[19]
#image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)
#resize image to appropriate dimensions
#image = cv2.resize(image,(224,224))
mask = mask.astype('uint8')
#apply the mask to the image
final = cv2.bitwise_and(image,image,mask = mask)
final = cv2.cvtColor(final,cv2.COLOR_BGR2RGB)
#plot the final image
plt.figure(figsize=(20,10))
plt.title("Image of Healthy individual's lung with only important features showing")
plt.imshow(final)
#plt.imshow(heatmap)